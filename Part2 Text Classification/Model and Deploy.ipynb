{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0db0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02650f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids('pos')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.words(movie_reviews.fileids('pos')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9814e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0][1], documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[-1][1], documents[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents), type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents[:100] + documents[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c361d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29163099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_review(review):\n",
    "    \"\"\"\n",
    "    Receives a raw review and clean it using the following steps:\n",
    "    1. Remove all non-words\n",
    "    2. Transform the review in lower case\n",
    "    3. Remove specific stop words\n",
    "    4. Perform stemming/lemma\n",
    "\n",
    "    Args:\n",
    "        review: the review that iwill be cleaned\n",
    "    Returns:\n",
    "        a clean review using the mentioned steps above.\n",
    "    \"\"\"\n",
    "    \n",
    "    review = re.sub(\"[^A-Za-z]\", \" \", review)\n",
    "    review = review.lower()\n",
    "    review = word_tokenize(review)\n",
    "#     review = [stemmer.stem(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "#     review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "    review = \" \".join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65031118",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" \".join(documents[0][0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62619e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_review(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_text,labels = [], []\n",
    "for doc, label in documents:\n",
    "    text = \" \".join(doc)\n",
    "    rev_text.append(clean_review(text))\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained word embeddings model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"This is a sentence.This is another sentence.\"\n",
    "doc = nlp(text_data)\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data to a matrix of word embeddings\n",
    "def get_features(rev_text):\n",
    "    feature_matrix = []\n",
    "    for document in rev_text:\n",
    "        doc = nlp(document)\n",
    "        feature_vector = doc.vector\n",
    "        feature_matrix.append(feature_vector)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = get_features(rev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f639741",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feature_matrix)\n",
    "\n",
    "# Convert the label data to numerical format\n",
    "y = np.array([1 if x==\"pos\" else 0 for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
    "\n",
    "# Define the deep learning model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=X.shape[1], output_dim=32, input_length=X.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af93f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ff48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array(get_features([\"This is a senetcne\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4aaf1",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2819132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json['data']\n",
    "    output = model.predict(np.array(get_features([data])))\n",
    "    return jsonify({'result': output.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3a50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b9cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
